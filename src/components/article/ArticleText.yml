tool:
  - >
    It is essential to understand how machine learning (ML) models make
    predictions in high-stakes settings such as healthcare, finance, and
    criminal justice. ML researchers have made great strides in developing
    interpretable models, and recent research focuses on
    <strong>operationalizing interpretability</strong> — leveraging an understanding of
    the domain to create more responsible and trustworthy ML systems.

  - >
    To help ML practitioners build trustworthy models, researchers have very
    recently developed a technique to generate the set of almost-optimal sparse
    interpretable decision trees. This set of high-performing models is called
    the <a target="_blank" href="https://arxiv.org/abs/2209.08040">Rashomon
    set</a>, named after the <a target="_blank"
    href="https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full">Rashomon
    effect</a> in statistics. Using the <a
    href='https://arxiv.org/abs/2209.08040' target='_blank'>TreeFARMS
    algorithm</a>, we can generate the whole Rashomon set of sparse decision
    trees. This set can contain <em>thousands of</em> inherently interpretable
    and almost-equally accurate models, providing opportunities for users to
    choose ones that best align with their knowledge and needs (e.g., fairness,
    monotonicity, simplicity). However, the large size of Rashomon sets and the
    diversity of contained models pose challenges for users to effectively
    explore and compare all these good models. TimberTrek is the first
    interactive visualization tool designed to tackle this critical challenge
    through <strong>summarizing the whole Rashomon set</strong> of sparse
    decision trees and empowering users to <strong>curate trees</strong> that meet their needs.

usage:
  p1: >
    With a flexible interface design, there are two options to use TimberTrek to
    explore your Rashomon sets. The first option is to select <a href='#top'
    style="font-variant: small-caps;">my own set</a> on the top of this page, and
    then upload your Rashomon trie data by dragging it to the interface (Video
    2-1). You can follow the instruction in the tool to learn how to generate
    the Rashomon JSON file.

  p2: >
    You can directly use TimberTrek in any computational notebook, such as
    Jupyter Notebook & Lab, Google Colab, and VS Code Notebook. To do that, you
    only need to install TimberTrek's Python package via <code>pip install
    timbertrek</code>. Then you can create a TimberTrek instance by passing your
    Rashomon trie data. With <a href="https://github.com/xiaohk/stickyland"
    target="_blank">Sticky Cells</a>, you can even create multiple
    repositionable TimberTrek windows and compare different subsets of a
    Rashomon set side by side! Try TimberTrek in the Jupyter Notebook below

tutorial:
  p1: >
    With multiple tightly integrated views, you can use TimberTrek to
    <em>explore</em>, <em>compare</em>, and <em>curate</em> sparse decision
    trees that align with your knowledge and values.
  items:
    - id: summary
      name: Rashomon Overview
      isWide: false
      descriptions:
        - >
          TimberTrek's primary view is a <a
          href='https://www.cc.gatech.edu/gvu/ii/sunburst/'
          target='_blank'>Sunburst</a> that summarizes all decision trees in the
          Rashomon set. We generate this <em>Rashomon Overview</em> by
          extracting all decision paths (path from the tree root to a leaf) from
          all decision trees in the Rashomon set. The Sunburst has \(k\) rings,
          where \(k\) is the height of the tallest tree in the Rashomon set.
          Each ring corresponds to a level of the decision path.
        - >
          Each ring is split into multiple sectors, and each sector corresponds
          to a testing condition of the decision tree. We color-code the sectors
          based on the feature attributes. For example, <span class="feature-tag" style="color:
          #4E79A7">number of prior crime</span>,  <span class="feature-tag" style="color:
          #F28E2C">age</span>, and <span class="feature-tag" style="color:
          #76B7B2">sex</span>. For features that have multiple values, we use color lightness
          to encode different values. For example, <span class="feature-tag" style="color:
          #4E79A7">prior > 3</span>,  <span class="feature-tag" style="color:
          #6B95C4">prior = 0</span>, and <span class="feature-tag" style="color:
          #88B1E2">prior = 1</span>. Finally, we use <span class="feature-tag" style="color:
          hsl(0,0%, 60%)">gray</span> to color encode <span class="feature-tag"
          style="color: hsl(0,0%, 60%)">leaf sectors</span> (end of a decision
          path). Each <span class="feature-tag" style="color: hsl(0,0%,
          60%)">leaf sector</span> is linked to a decision tree that has the
          corresponding decision path. Sectors are positioned based on their
          hierarchical relationship in the decision rule. For example, if we
          have two sectors aligned as <span class="feature-tag" style="color:
          #4E79A7">prior > 3</span>,  <span class="feature-tag" style="color:
          #F28E2C">age < 26</span>, it means all decision rules after these two
          sectors use <span class="feature-tag" style="color: #4E79A7">prior >
          3</span> as the first split and <span class="feature-tag"
          style="color: #F28E2C">age < 26</span> as the second split.
        - >
          You can click a sector to "zoom into" a subset of decision trees with
          the selected decision path patterns; the Sunburst only
          displays decision rules with a feature order up to the clicked sector.
          You can hover over a <span class="feature-tag" style="color: hsl(0,0%,
          60%)">leaf sector</span> to see a preview of its linked decision tree.
          Some Rashomon sets can have very tall trees. Therefore, you can use the
          depth menu (on the top left) to control how many levels to show in the
          Sunburst; it is helpful to focus on a few levels at a time.
      caption: The <em>Rashomon Overview</em> adapts Sunburst to summarize the
        whole Rashomon set. It organizes decision trees based on their decision
        paths. Users can click a sector to zoom into a specific subset of models
        with similar prediction patterns. Users can also control the number of
        levels to show in the Sunburst.
    - id: tree
      name: Tree Window
      isWide: true
      descriptions:
        - >
          When you click a <span class="feature-tag" style="color: hsl(0,0%,
          60%)">leaf sector</span>, a <em>Tree Window</em> appears to present
          details for the linked decision tree. This window uses a node-link
          diagram to visualize the tree structure by default. This window
          uses stroke opacity to encode the accuracies of each leaf node in the
          tree; when you hover over a leaf node, you can also inspect the number
          of samples that the leaf node affects. In addition, you can
          click the toggle switch on the bottom right to switch the tree
          visualization to a waterfall-like plot, where the size of each node is
          augmented by the training samples that pass through them.
        - >
          You can open multiple <em>Tree Windows</em> at once, and drag windows
          into different locations. Arranging multiple windows side by side
          could help you compare the structures and prediction patterns of
          interested model candidates.
      caption: >
        A <em>Tree Windows</em> uses a node-link diagram to illustrate the
        details of a decision tree. An alternative waterfall-like chart
        augments node size with its sample size, helping users to better
        understand each tree's robustness. To compare the structure and
        prediction patterns of multiple decision trees, users can open multiple
        windows and drag to reposition them side by side.
    - id: search
      name: Search Panel
      isWide: true
      descriptions:
        - >
          Want to quickly identify trees with desired properties? The <em>Search
          Panel</em> comes to the rescue! This panel provides a suite of
          filtering tools that control what decision trees to include in the
          Sunburst. For example, you can drag the "Minimum Leaf Sample" slider
          to tell the <em>Rashomon Overview</em> to only display decision trees
          whose leaves have more samples. Similarly, you can click the checkboxes
          to select what features to include in the tree or at a particular
          level. Adjusting the filtering setting would trigger the <em>Rashomon
          Overview</em> to animate to display/hide affected sectors.
      caption: >
        The <em>Search Panel</em> allows users to search for decision trees with
        desired properties by applying different filtering tools. For example, a
        user can drag the accuracy slider to hide decision trees with slightly
        smaller accuracy scores. Similarly, a user can also click checkboxes under
        "Depth 3" to search for trees that use specific features in their
        second level.
    - id: favorite
      name: Favorite Panel
      isWide: true
      descriptions:
        - >
          Once you have found decision trees that meet your needs, you can click
          the heart button on the top right of the <em>Tree Window</em> to
          bookmark them. You can see all bookmarked trees in the <em>Favorite
          Panel</em>. You can also click the comment button on the <em>Tree
          Window</em> to add a short comment to document why you choose this
          decision tree. Alternatively, you can add or edit comments in the
          <em>Favorite Panel</em> directly. Once you are satisfied with your
          decision tree curation, you can click the download button to save all
          bookmarked trees with your comments. You can load these trees with
          TimberTrek's Python package. Your comments can also help you continue
          curating in the future.
      caption: >
        Users can use the <em>Favorite Panel</em> to keep track of all
        bookmarked decision trees. This panel shows a thumbnail of each tree
        along with a user-entered comment. The comment can document the model
        curation motivation and context. Once satisfied with their curation, a
        user can click the save button to download all bookmarked trees with
        their documentation. It helps users deploy satisfactory models or
        continue curation in the future.

development: >
  TimberTrek is written in <a href='https://www.typescriptlang.org'
  target='_blank'>TypeScript</a> using <a href='https://svelte.dev'
  target='_blank'>Svelte</a> as a framework and <a href='https://d3js.org'
  target='_blank'>D3.js</a> for visualizations. The source code is available on
  <a href='https://github.com/poloclub/timbertrek' target='_blank'>GitHub</a>.
  The entire app runs locally and privately in your web browser, and your model
  and data would never leave your machine. Anyone can easily access TimberTrek
  on their desktop or tablets, without the need of setting up dedicated servers or
  writing code. With a flexible design, TimberTrek supports all types of
  computational notebooks, such as Jupyter Notebook & Lab, Google Colab, and VS
  Code Notebook. With the cross-platform support, we hope TimberTrek will fit into
  your favorite ML development workflow.

team: >
  Led by <a href='https://zijie.wang/' target='_blank'>Jay Wang</a>, TimberTrek
  is a result of a collaboration between ML and visualization researchers from
  Georgia Tech, Duke University, Fujitsu Laboratories, and University of British
  Columbia. TimberTrek is created by <a href='https://zijie.wang/'
  target='_blank'>Jay Wang</a>, <a href='https://www.linkedin.com/in/chudizhong'
  target='_blank'>Chudi Zhong</a>, <a
  href='https://www.linkedin.com/in/rui-xin-8070181b9' target='_blank'>Rui
  Xin</a>, <a
  href='https://scholar.google.com/citations?user=9fY1WVIAAAAJ&hl=en'
  target='_blank'>Takuya Takagi</a>, <a
  href='https://users.cs.duke.edu/~zhichen/' target='_blank'>Zhi Chen</a>, <a
  href='https://www.cc.gatech.edu/~dchau' target='_blank'>Polo Chau</a>, <a
  href='https://users.cs.duke.edu/~cynthia/' target='_blank'>Cynthia Rudin</a>,
  and <a href='https://www.seltzer.com/margo/' target='_blank'>Margo
  Seltzer</a>.

contribute:
  - >
    If you have any questions or feedback, feel free to <a
    href='https://github.com/poloclub/timbertrek/issues' target='_blank'>open an
    issue</a> or contact <a href='https://zijie.wang/' target='_blank'>Jay
    Wang</a>.
  - >
    We’d love to hear your experience with TimberTrek! If you’d like to
    share (e.g., what is your typical workflow to curate decision trees, what
    features of TimberTrek you find most helpful), please reach out to us.
    TimberTrek is an open-source project, and we welcome <a
    href='https://github.com/poloclub/timbertrek/pulls' target='_blank'>pull
    requests</a> for new feature implementations and bug fixes, etc.

youtubeTimes:
  - startTime: 0
    name: Sparse decision tree
    timestamp: (0:00-0:06)
  - startTime: 6
    name: Rashomon set
    timestamp: (0:06-0:38)
  - startTime: 38
    name:  Introducing TimberTrek
    timestamp: (0:38-0:45)
  - startTime: 45
    name: Rashomon overview
    timestamp: (0:45-1:13)
  - startTime: 73
    name: Tree windows
    timestamp: (1:13-1:33)
  - startTime: 93
    name: Search Panel
    timestamp: (1:33-2:02)
  - startTime: 122
    name: Searching for fair models
    timestamp: (2:02-2:41)
  - startTime: 161
    name: Computational notebook support
    timestamp: (2:41-2:49)
  - startTime: 129
    name: Try TimberTrek
    timestamp: (2:49-3:00)

cite:
  intro: >
    To learn more about TimberTrek, please read our <a
    href='https://arxiv.org/abs/2209.09227' target='_blank'>research paper</a>
    (published at <a href='https://ieeevis.org/year/2022/welcome'
    target='_blank'>IEEE VIS'22</a>). To learn more about the algorithm to
    generate the whole Rashomon set of sparse decision trees, please read our <a
    href='https://arxiv.org/abs/2209.08040' target='_blank'>TreeFARMS paper</a>
    (published at NeurIPS'22). If you find TimberTrek useful for your research,
    please consider citing our paper. Thanks!
  bibtex: >
    @inproceedings{wangTimberTrekExploringCurating2022,
      title = {{{TimberTrek}}: {{Exploring}} and {{Curating Trustworthy Decision Trees}} with {{Interactive Visualization}}},
      booktitle = {2022 {{IEEE Visualization Conference}} ({{VIS}})},
      author = {Wang, Zijie J. and Zhong, Chudi and Xin, Rui and Takagi, Takuya and Chen, Zhi and Chau, Duen Horng and Rudin, Cynthia and Seltzer, Margo},
      year = {2022}
    }
  title: >
    TimberTrek: Exploring and Curating Sparse Decision Trees with Interactive
    Visualization
  paperLink: >
    https://arxiv.org/abs/2209.09227
  venue: >
    IEEE Visualization Conference (VIS) 2022
  venueLink: >
    https://ieeevis.org/year/2022/welcome
  authors:
    - name: Zijie J. Wang
      url: 'https://zijie.wang/'
    - name: Chudi Zhong
      url: 'https://www.linkedin.com/in/chudizhong'
    - name: Rui Xin
      url: 'https://www.linkedin.com/in/rui-xin-8070181b9'
    - name: Takuya Takagi
      url: 'https://scholar.google.com/citations?user=9fY1WVIAAAAJ&hl=en'
    - name: Zhi Chen
      url: 'https://users.cs.duke.edu/~zhichen/'
    - name: Polo Chau
      url: 'https://www.cc.gatech.edu/~dchau'
    - name: Cynthia Rudin
      url: 'https://users.cs.duke.edu/~cynthia/'
    - name: Margo Seltzer
      url: 'https://www.seltzer.com/margo/'